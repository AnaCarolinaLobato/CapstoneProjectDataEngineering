{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Data Engineering Capstone Project\n\n## Project Summary\nThe Data Engineering Capstone Project represents a comprehensive data engineering initiative, involving the collection, processing, and modeling of data from diverse sources to construct a valuable and insightful data model. The primary dataset for this project encompasses information on immigration to the United States, supplemented by datasets covering airport codes and U.S. city demographics.\n\nThe project follows the follow steps:\n\n- Step 1: Scope the Project and Gather Data\n- Step 2: Explore and Assess the Data\n- Step 3: Define the Data Model\n- Step 4: Run ETL to Model the Data\n- Step 5: Complete Project Write Up",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Do all imports and installs here\nimport pandas as pd\nimport os\nimport configparser\nfrom datetime import datetime, timedelta\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\nfrom pyspark.sql.functions import udf,col, array_contains, split,monotonically_increasing_id\nfrom pyspark.sql.types import DateType, StringType\nfrom pyspark.sql import SQLContext\n\nconfig = configparser.ConfigParser()\nconfig.read('aws.cfg')\n\nos.environ['AWS_ACCESS_KEY_ID']=config['AWS_CREDENTIALS']['AWS_ACCESS_KEY_ID']\nos.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_CREDENTIALS']['AWS_SECRET_ACCESS_KEY']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 1: Scope the Project and Gather Data\n\n### Scope\nIn this project, we aim to build a data pipeline for analyzing immigration data to the United States, alongside supplementary datasets containing information about airport codes, U.S. city demographics, and temperature data. Our goal is to extract, transform, and load the data into a structured format that can be used for various analytical purposes. We will use Apache Spark for data processing and transformation.\n\n### Describe and Gather Data\nImmigration Data: The immigration data originates from the U.S. National Tourism and Trade Office and covers international visitor arrivals in the USA. We will work with data from April 2016.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Read immigration data using pandas\npath_i94 = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\ndf_i94 = pd.read_sas(path_i94, 'sas7bdat', encoding=\"ISO-8859-1\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Display a sample of the data\ndf_i94.head(5)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Read airport data\ndf_airport = pd.read_csv(\"./airport-codes_csv.csv\")\ndf_airport.head(5)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Read U.S. city demographics data\ndf_demographics = pd.read_csv(\"./us-cities-demographics.csv\", delimiter=\";\")\ndf_demographics.head(5)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Set up a Spark session\nspark = SparkSession.builder.\\\nconfig(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\nconfig(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\nenableHiveSupport().getOrCreate()\n\n# Read immigration data into a Spark DataFrame and write to Parquet\ndf_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\ndf_spark.write.mode(\"overwrite\").parquet(\"sas_data\")\ndf_spark = spark.read.parquet(\"sas_data\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: Explore and Assess the Data\n\n### Explore the Data\nIn this step, will identify data quality issues such as missing values, duplicate data, etc., and explore the datasets.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Explore the immigration data\ndf_immigration.describe()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Explore the airport data\ndf_airport.describe()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Explore the demographics data\ndf_demographics.describe()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Cleaning Steps\nwill perform data cleaning tasks to address the data quality issues. These tasks include:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get port locations from SAS text file\n# Remove irregular ports and NaN values\ndf_i94_filtered = df_i94[~df_i94[\"i94port\"].isin(irr_ports)]\ndf_i94_filtered.drop(columns=[\"insnum\", \"entdepu\", \"occup\", \"visapost\"], inplace=True)\ndf_i94_filtered.dropna(inplace=True)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Remove rows with missing iata_code\ndf_airport.dropna(subset=['iata_code'], inplace=True)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Convert SAS date to a more accurate date format\nimmigration_fact['arrival_date'] = immigration_fact.arrival_date.apply(lambda x: sas_to_date(x))\nimmigration_fact['departure_date'] = immigration_fact.departure_date.apply(lambda x: sas_to_date(x))\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: Define the Data Model\n### 3.1 Conceptual Data Model\nwill use a star schema model for the data structure, which provides a simple and clear structure for data analysis and understanding results. The data model will consist of the following tables:\n\nFact Table:\n- Immigration Fact Table: Contains information related to immigration, including arrival and departure dates, mode of transportation, and visa type.\n\nDimension Tables:\n- Demographics Dimension Table: Contains demographic information about U.S. cities.\n- Airports Dimension Table: Contains information about airports.\n\nThe star schema allows for easy analysis and query optimization.\n\n### 3.2 Mapping Out Data Pipelines\nTo create the data pipelines, follow these steps:\n\n- Enter your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY into the keys.cfg file.\n- Run the etl.py script to trigger a Spark job that processes and transforms the data into a combination of facts and dimension tables.\n- Check the output in your S3 data lake to confirm a successful load.\n\n## Step 4: Run Pipelines to Model the Data\n### 4.1 Create the Data Model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def createsparksession():\n    spark = SparkSession.builder\\\n        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n        .enableHiveSupport().getOrCreate()\n\n    return spark\n\ndef read_labels(file, first_row, last_row):\n    frame = {}\n    frame2 = {}\n    print(first_row)\n    with open(file) as f:\n        file_content = f.readlines()\n\n        for content in file_content[first_row:last_row]:\n            content = content.split(\"=\")\n            if first_row == 303:\n                code, cont = content[0].strip(\"\\t\").strip().strip(\"'\"), content[1].strip(\"\\t\").strip().strip(\"''\")\n            else:\n                code, cont = content[0].strip(), content[1].strip().strip(\"'\")\n            frame[code] = cont\n    return frame\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def process_immigration_data(spark, input_data, output_data):\n    \"\"\"\n    Loads immigration data and processes it into a fact table\n    input_data: source of data\n    output_data: destination storage for processed data\n    \"\"\"\n    # read/load immigration_data file\n    immigration_data = os.path.join(input_data + \"sas_data\")\n\n    # load data\n    df_immigration = spark.read.parquet(immigration_data)\n\n    # extract columns to create a table\n    df_immigration = df_immigration.select(\"*\")\n\n    # rename columns to make them intelligible\n    rename_cols = [\n        \"cic_id\",\n        \"year\",\n        \"month\",\n        \"cit\",\n        \"res\",\n        \"port\",\n        \"arrival_date\",\n        \"mode\",\n        \"address\",\n        \"departure_date\",\n        \"age\",\n        \"visa\",\n        \"count\",\n        \"date_logged\",\n        \"dept_visa_issuance\",\n        \"occupation\",\n        \"arrival_flag\",\n        \"departure_flag\",\n        \"update_flag\",\n        \"match_flag\",\n        \"birth_year\",\n        \"max_stay_date\",\n        \"gender\",\n        \"INS_number\",\n        \"airline\",\n        \"admission_number\",\n        \"flight_number\",\n        \"visatype\",\n    ]\n\n    # create an iterator between columns\n    for col, rename_col in zip(df_immigration.columns, rename_cols):\n        df_immigration = df_immigration.withColumnRenamed(col, rename_col)\n\n    immigration_fact = df_immigration[\n        [\n            \"cic_id\",\n            \"year\",\n            \"month\",\n            \"arrival_date\",\n            \"departure_date\",\n            \"mode\",\n            \"visatype\",\n        ]\n    ].dropDuplicates(['cic_id'])\n\n    # dimensions from immigration data\n    dim_flight_details = df_immigration.select([monotonically_increasing_id().alias('id'), 'cic_id', 'flight_number', 'airline']).dropDuplicates()\n    dim_immigrants = df_immigration.select([monotonically_increasing_id().alias('id'), 'cic_id', 'cit', 'res', 'visa', 'age', 'occupation', 'gender', 'address', 'INS_number']).dropDuplicates()\n\n    # organizing data\n    udf_func = udf(sas_to_date, DateType())\n    immigration_fact = immigration_fact.withColumn(\"arrival_date\", udf_func(\"arrival_date\"))\n    immigration_fact = immigration_fact.withColumn(\"departure_date\", udf_func(\"departure_date\"))\n    file = os.path.join(input_data + \"I94_SAS_Labels_Descriptions.SAS\")\n    countries = read_labels(file, first_row=10, last_row=298)\n    cities = read_labels(file, first_row=303, last_row=962)\n    countries_df = spark.createDataFrame(countries.items(), ['code', 'country']).dropDuplicates()\n    cities_df = spark.createDataFrame(cities.items(), ['code', 'city']).dropDuplicates()\n    cities_df = cities_df.withColumn('state', split(cities_df.city, ',').getItem(1))\\\n            .withColumn('city', split(cities_df.city, ',').getItem(0))\n    cities_df = cities_df.select([monotonically_increasing_id().alias('id'), '*'])\n    countries_df = countries_df.select([monotonically_increasing_id().alias('id'), '*'])\n\n    # write parquet\n    countries_df.write.parquet(output_data + \"countries_df.parquet\")\n    cities_df.write.parquet(output_data + \"cities_df.parquet\")\n    immigration_fact.write.mode('overwrite').parquet(output_data + \"fact_immigration.parquet\")\n    dim_immigrants.write.parquet(output_data + \"dim_immigrants.parquet\")\n    dim_flight_details.write.parquet(output_data + \"dim_flight_details.parquet\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def process_demographics_data(spark, input_data, output_data):\n    \"\"\"\n    loads and processes city data\n    input_data: path to data source\n    output_data: path to data store\n    \"\"\"\n\n    # fetch data path\n    city_data = os.path.join(input_data + \"us-cities-demographics.csv\")\n\n    # load city data\n    city_df = spark.read.load(city_data, sep=\";\", format=\"csv\", header=True)\n\n    # Demography, examine the city based on size, and structure (Gender & Ethnicity composition)\n    city_demography = city_df.select(\n        [\n            \"City\",\n            \"State\",\n            \"State Code\",\n            \"Male Population\",\n            \"Female Population\",\n            \"Foreign-born\",\n            \"Number of Veterans\",\n            \"Race\",\n        ]\n    ).dropDuplicates()\n    city_demography.write.parquet(output_data + \"city_demography.parquet\")\n\n    # statistics on city such as total population, and median age\n    city_stats = city_df.select(\n        [\"City\", \"State Code\", \"Median Age\", \"Average Household Size\", \"Count\"]\n    ).dropDuplicates()\n    city_stats.write.mode('overwrite').parquet(output_data + \"city_stats.parquet\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def process_airport_data(spark, input_data, output_data):\n    \"\"\"\n    loads and processes city data\n    input_data: path to data source\n    output_data: path to data store\n    \"\"\"\n\n    # fetch data source\n    airport_data = os.path.join(input_data + \"airport-codes_csv.csv\")\n    \n    # load data\n    airport_df = spark.read.csv(airport_data, header=True)\n    airport_df = airport_df.filter(\n        (airport_df.iso_country == \"US\") & ~(airport_df.type == \"closed\")\n    ).dropDuplicates()\n    \n    # data quality check (unique identity column)\n    check_unique_id_column(airport_df, 'idents')\n    \n    airport_dim = airport_df.select([\"ident\", \"type\", \"name\", \"continent\", \"gps_code\", \"iata_code\", \"local_code\", \"iso_country\"]).dropDuplicates()\n    airport_stats = airport_df.select([\"ident\", \"elevation_ft\", \"coordinates\"]).dropDuplicates()\n\n    airport_dim.write.parquet(output_data + \"airports_dim.parquet\")\n    airport_stats.write.parquet(output_data + \"airports_stats.parquet\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 4.2 Data Quality Checks\nExplain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n\n- Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n- Unit tests for the scripts to ensure they are doing the right thing\n- Source/Count checks to ensure completeness\n\n### Run Quality Checks\n- cities_dtype = [('id', 'bigint'), ('code', 'string'), ('city', 'string'), ('state', 'string')]\n- countries_dtype = [('id', 'bigint'), ('code', 'string'), ('country', 'string')]\n- city_demography_dtype = [('City', 'string'), ('State', 'string'), ('State Code', 'string'), ('Male Population', 'string'), ('Female Population', 'string'), ('Foreign-born', 'string'), ('Number of Veterans', 'string'), ('Race', 'string')]\n- city_stats_dtype = [('City', 'string'), ('State Code', 'string'), ('Median Age', 'string'), ('Average Household Size', 'string'), ('Count', 'string')]\n- airports_stats_dtype = [('ident', 'string'), ('elevation_ft', 'string'), ('coordinates', 'string')]\n- airports_dim_dtype = [('ident', 'string'), ('type', 'string'), ('name', 'string'), ('continent', 'string'), ('gps_code', 'string'), ('iata_code', 'string'), ('local_code', 'string'), ('iso_country', 'string')]\n- fact_immigration_dtype = [('cic_id', 'double'), ('year', 'double'), ('month', 'double'), ('arrival_date', 'date'), ('departure_date', 'date'), ('mode', 'double'), ('visatype', 'string')]",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "def check_schema(spark, output_path):\n    print(Path(output_path))\n    output_path = Path(output_path)\n    for file in output_path.iterdir():\n        if file.is_dir():\n            if file == 'fact_immigration.parquet' and (spark.read.parquet(str(file)).dtypes != fact_immigration_dtype):\n                raise ValueError(\"Fact Immigration Table schema not correct\")\n            elif file == 'airports_dim.parquet' and (spark.read.parquet(str(file)).dtypes != airports_dim_dtype):\n                raise ValueError(\"Airports Dim Table schema not correct\")\n            elif file == 'airports_stats.parquet' and (spark.read.parquet(str(file)).dtypes != airports_stats_dtype):\n                raise ValueError(\"Airports Stats Table schema not correct\")\n            elif file == 'city_stats.parquet' and (spark.read.parquet(str(file)).dtypes != city_stats_dtype):\n                raise ValueError(\"City Stats Table schema not correct\")\n            elif file == 'city_demography.parquet' and (spark.read.parquet(str(file)).dtypes != city_demography_dtype):\n                raise ValueError(\"City Demography Table schema not correct\")\n            elif file == 'countries.parquet' and (spark.read.parquet(str(file)).dtypes != countries_dtype):\n                raise ValueError(\"Countries Table schema not correct\")\n            elif file == 'cities.parquet' and (spark.read.parquet(str(file)).dtypes != cities_dtype):\n                raise ValueError(\"Cities Table schema not correct\")\n            else:\n                \"Table Schemas are correct\"\n          \n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def check_empty_tables(spark, output_path):\n    output_path = Path(output_path)\n    for file in output_path.iterdir():\n        if file.is_dir():\n            df = spark.read.parquet(str(file)).count()\n            if df == 0:\n                print(\"Empty Table: \", df.split('/')[-1])\n            else:\n                print(\"Table: \", df.split('/')[-1], \"is not empty\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 4.3 Data dictionary\nCreate a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n\n## Step 5: Complete Project Write Up\n- Clearly state the rationale for the choice of tools and technologies for the project.\n- Propose how often the data should be updated and why.\n- Write a description of how you would approach the problem differently under the following scenarios:\n- The data was increased by 100x.\n- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n- The database needed to be accessed by 100+ people.",
      "metadata": {}
    }
  ]
}